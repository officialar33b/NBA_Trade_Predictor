---
title: "DATA 467 Project Data Analysis and Linear Model Selection"
output:
  html_document:
    df_print: paged
---

#Data Analysis and Visualization
```{r}
# Loading the dataframes.
basketball_data <- read.csv('train.csv')

cols_list <- c('RANK', 'TEAM_ID', 'GP', 'MIN', 'FGM', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'EFF', 'AST_TOV', 'STL_TOV', 'CHANGED_FRANCHISE', 'SEASON_ID')

basketball_data <- basketball_data[, cols_list]
```

```{r}
summary(basketball_data)
```
```{r}
table(basketball_data$CHANGED_FRANCHISE)
barplot(table(basketball_data$CHANGED_FRANCHISE),
        names.arg = c("Stayed", "Changed"), col = "lightblue", 
        main = "Distribution of Franchise Changes")

```
```{r}
hist(basketball_data$PTS, main = "Distribution of Points per Game", xlab = "Points")
hist(basketball_data$EFF, main = "Player Efficiency Distribution", xlab = "EFF")
```



#Logistic Regression Model
We decided to use a logistic regression because our response variable is binomial, a player changes franchise or does not change franchise.
```{r}
library('tidyverse')

# Loading the dataframes.
train_df <- read.csv('train.csv')

#head(train_df, 10)

train_copy <- train_df

# Selecting desired columns.
cols_list <- c('RANK', 'TEAM_ID', 'GP', 'MIN', 'FGM', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'EFF', 'AST_TOV', 'STL_TOV', 'CHANGED_FRANCHISE', 'SEASON_ID')
train_df <- train_df[, cols_list]
#head(train_df, 10)

# Convert to factor.
train_df$CHANGED_FRANCHISE <- as.factor(train_df$CHANGED_FRANCHISE)
train_df$SEASON_ID <- as.factor(train_df$SEASON_ID)

# Check for missing values.
#sum(is.na(train_df))

#Logistic regression model
logit_mod <- glm(CHANGED_FRANCHISE ~ ., family=binomial(link='logit'), data = train_df)
```

```{r}
summary(logit_mod)
```
##Diagnostics For Logistic Regression
```{r}
#Residuals
train_df <- mutate(train_df,
                   residuals = residuals(logit_mod),
                   linpred = predict(logit_mod))

gdf <- group_by(train_df,
                bin = cut(linpred, breaks = unique(quantile(linpred, (1:100)/101))))

diagdf <- summarise(gdf,
                    residuals = mean(residuals),
                    linpred = mean(linpred))

plot(residuals ~ linpred, diagdf, xlab = "linear predictor")
```
Because there is not a random scatter in our fitted versus residual plot, we can say that this model does not meet our assumption of linearity.

```{r}
#Half norm plot
library(faraway)
halfnorm(hatvalues(logit_mod))
```
From the half norm plot, we can see that there are only two outliers.

##Model Comparison
```{r}
#Smaller model from ommitting non significant predictors
columns <- c('RANK', 'TEAM_ID', 'FG3M', 'FG3A', 'DREB', 'BLK', 'EFF', 'AST_TOV', 'CHANGED_FRANCHISE')
new_train <- train_df[, columns]

new_train$CHANGED_FRANCHISE <- as.factor(new_train$CHANGED_FRANCHISE)

logit_mod_reduced <- glm(CHANGED_FRANCHISE ~ ., family=binomial(link='logit'), data = new_train)
#Summary of new log mod
summary(logit_mod_reduced)
```
```{r}
#Compare the reduced vs full models
anova(logit_mod_reduced, logit_mod, test = "Chisq")
```
The p-value is 2.2e-16 which is extremely small. This means that the additional predictors in Model 2(the full model) significantly improve the fit of the model and help explain the response variable(changed franchise).

##Hypothesis Test
Null Hypothesis($H_0$): The reduced model(Model 1) fits just as well as the full model(Model 2) and the additional predictors in the full model do not improve the model.
Alternative Hypothesis($H_1$): The fulle model(Model 2) is better fit than the reduced model(Model 1).
alpha = 0.05

D = $Deviance_{Model1}$ - $Deviance_{Model2}$ = 16681 âˆ’ 16225 = 456.43
D = 456.43
df = 42
p-value = 2.2e-16

Conclusion:
Since the p-value is extremely small, we reject the null hypothesis. This indicates that the full model offers a significantly improved fit over the reduced model, and the additional predictors included in the full model are meaningful in explaining the response variable.

